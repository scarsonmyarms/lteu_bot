# import requests
# from bs4 import BeautifulSoup
# import fake_useragent
# import re
# import os
# import json
# from datetime import datetime, timedelta
# import logging
#
# # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ª–æ–≥—É–≤–∞–Ω–Ω—è
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
#
# CACHE_DIR = 'cache'
# CACHE_EXPIRATION_TIME = timedelta(hours=1)
#
# def _sanitize_filename(url: str) -> str:
#     """–û—á–∏—â—É—î URL –¥–ª—è –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –≤ —è–∫–æ—Å—Ç—ñ —ñ–º–µ–Ω—ñ —Ñ–∞–π–ª—É."""
#     return url.replace('https://', '').replace('/', '_').replace(':', '_').replace('?', '_').replace('=', '_') + '.json'
#
# def _fetch_html(url: str) -> str:
#     """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î HTML-–∫–æ–Ω—Ç–µ–Ω—Ç –∑ –≤–∫–∞–∑–∞–Ω–æ–≥–æ URL –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º –∫–µ—à—É–≤–∞–Ω–Ω—è."""
#     os.makedirs(CACHE_DIR, exist_ok=True)
#     cache_file = os.path.join(CACHE_DIR, _sanitize_filename(url))
#
#     if os.path.exists(cache_file):
#         try:
#             with open(cache_file, 'r') as f:
#                 cache_data = json.load(f)
#                 if datetime.now() - datetime.fromisoformat(cache_data['timestamp']) < CACHE_EXPIRATION_TIME:
#                     logging.info(f"–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –∫–µ—à–æ–≤–∞–Ω–∞ –≤–µ—Ä—Å—ñ—è –¥–ª—è {url}")
#                     return cache_data['content']
#                 else:
#                     logging.info(f"–ö–µ—à –∑–∞—Å—Ç–∞—Ä—ñ–≤ –¥–ª—è {url}, –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –Ω–æ–≤—ñ –¥–∞–Ω—ñ")
#         except (FileNotFoundError, json.JSONDecodeError):
#             logging.warning(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —á–∏—Ç–∞–Ω–Ω—ñ –∫–µ—à-—Ñ–∞–π–ª—É –¥–ª—è {url}, –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –Ω–æ–≤—ñ –¥–∞–Ω—ñ")
#
#     user = fake_useragent.UserAgent().random
#     headers = {'user-agent': user}
#     try:
#         response = requests.get(url, headers=headers, timeout=10)
#         response.raise_for_status()  # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –ø–æ–º–∏–ª–∫–∏ HTTP
#         content = response.text
#         cache_data = {'timestamp': datetime.now().isoformat(), 'content': content}
#         with open(cache_file, 'w') as f:
#             json.dump(cache_data, f)
#         return content
#     except requests.exceptions.RequestException as e:
#         logging.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—ñ {url}: {e}")
#         return ""
#
# def _extract_phone_numbers(text: str) -> list[str]:
#     """–í–∏—Ç—è–≥—É—î –Ω–æ–º–µ—Ä–∏ —Ç–µ–ª–µ—Ñ–æ–Ω—ñ–≤ —É —Ñ–æ—Ä–º–∞—Ç—ñ +38 –∑ —Ç–µ–∫—Å—Ç—É."""
#     return re.findall(r'\+38 \d{9}', text)
#
# def parse_priyomna_numbers() -> list[str]:
#     """–ü–∞—Ä—Å–∏—Ç—å –Ω–æ–º–µ—Ä–∏ —Ç–µ–ª–µ—Ñ–æ–Ω—ñ–≤ –ø—Ä–∏–π–º–∞–ª—å–Ω–æ—ó –∫–æ–º—ñ—Å—ñ—ó."""
#     url = "https://www.lute.lviv.ua/admissions/priimalna-komisija/?L=274"
#     html = _fetch_html(url)
#     if not html:
#         return []
#     soup = BeautifulSoup(html, 'html.parser')
#     phone_numbers = []
#     for p in soup.select('p.bodytext'):
#         phone_numbers.extend(_extract_phone_numbers(p.get_text()))
#     logging.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ –Ω–æ–º–µ—Ä–∏ —Ç–µ–ª–µ—Ñ–æ–Ω—ñ–≤ –ø—Ä–∏–π–º–∞–ª—å–Ω–æ—ó –∫–æ–º—ñ—Å—ñ—ó: {phone_numbers}")
#     return phone_numbers
#
# def parse_priyomna_info() -> dict[str, str]:
#     """–ü–∞—Ä—Å–∏—Ç—å –∫–æ–Ω—Ç–∞–∫—Ç–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–∏–π–º–∞–ª—å–Ω–æ—ó –∫–æ–º—ñ—Å—ñ—ó (–∞–¥—Ä–µ—Å–∞, email, —Å–æ—Ü–º–µ—Ä–µ–∂—ñ)."""
#     url = "https://www.lute.lviv.ua/admissions/priimalna-komisija/?L=274"
#     html = _fetch_html(url)
#     if not html:
#         return {}
#     soup = BeautifulSoup(html, 'html.parser')
#     contact_box = soup.find('div', class_='fce-box-orange')
#     if not contact_box:
#         logging.warning("–ë–ª–æ–∫ –∫–æ–Ω—Ç–∞–∫—Ç–Ω–æ—ó —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.")
#         return {}
#
#     address_span = contact_box.find('span', lang='RU')
#     address = address_span.get_text(strip=True).replace('–∞–¥—Ä–µ—Å–∞:', '').strip() if address_span else None
#
#     email_link = contact_box.find('a', class_='mail')
#     email = email_link.get_text(strip=True).replace('(at)', '@') if email_link else None
#
#     facebook_link = contact_box.find('a', href=re.compile(r'facebook\.com'))
#     facebook = facebook_link.get('href') if facebook_link else None
#
#     telegram_link = contact_box.find('a', href=re.compile(r't\.me'))
#     telegram = telegram_link.get('href') if telegram_link else None
#
#     result = {
#         'address': address,
#         'email': email,
#         'facebook': facebook,
#         'telegram': telegram
#     }
#     logging.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –ø—Ä–∏–π–º–∞–ª—å–Ω—É –∫–æ–º—ñ—Å—ñ—é: {result}")
#     return result
#
# def get_rektor() -> dict[str, str]:
#     """–ü–∞—Ä—Å–∏—Ç—å –∫–æ–Ω—Ç–∞–∫—Ç–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é —Ä–µ–∫—Ç–æ—Ä–∞."""
#     url = "https://www.lute.lviv.ua/universitet/kontakti/"
#     html = _fetch_html(url)
#     if not html:
#         return {}
#     soup = BeautifulSoup(html, 'html.parser')
#     rektor_divs = soup.select('#c5228')
#     if not rektor_divs:
#         logging.warning("–ë–ª–æ–∫ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –ø—Ä–æ —Ä–µ–∫—Ç–æ—Ä–∞ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.")
#         return {}
#
#     def _process_contact_data(divs: list) -> dict[str, str]:
#         """–û–±—Ä–æ–±–ª—è—î —Ç–∞ —Ñ–æ—Ä–º–∞—Ç—É—î –∫–æ–Ω—Ç–∞–∫—Ç–Ω—ñ –¥–∞–Ω—ñ."""
#         data_dict = {}
#         for div in divs:
#             for p in div.find_all('p'):
#                 text = p.text.strip().replace('\xa0', ' ')
#                 if ":" in text:
#                     key, value = text.split(":", 1)
#                     key = key.strip()
#                     value = value.strip()
#                     if key == '—Ç–µ–ª–µ—Ñ–æ–Ω':
#                         phones = re.findall(r'\(\d{3}\) \d{3}-\d{2}-\d{2}', value)
#                         data_dict[key] = ', '.join(phones)
#                     else:
#                         data_dict[key] = value
#         return data_dict
#
#     rektor_info = _process_contact_data(rektor_divs)
#     logging.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ —Ä–µ–∫—Ç–æ—Ä–∞: {rektor_info}")
#     return rektor_info
#
# def parse_hostel_info() -> dict[str, list[str]]:
#     """–ü–∞—Ä—Å–∏—Ç—å —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –≥—É—Ä—Ç–æ–∂–∏—Ç–∫–∏."""
#     url = "https://www.lute.lviv.ua/admissions/prozhivannja-studentiv/?L=458"
#     html = _fetch_html(url)
#     if not html:
#         return {}
#     soup = BeautifulSoup(html, 'html.parser')
#     content_div = soup.find('div', class_='csc-textpic-text')
#     if not content_div:
#         logging.warning("–ë–ª–æ–∫ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –ø—Ä–æ –≥—É—Ä—Ç–æ–∂–∏—Ç–∫–∏ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.")
#         return {}
#
#     first_paragraph = content_div.find('p', class_='bodytext').get_text(strip=True) if content_div.find('p', class_='bodytext') else None
#     hostel_info = []
#     for li in content_div.find_all('li', limit=3):
#         text = li.get_text(strip=True).replace('\xa0', ' ')
#         text = re.sub(r', –∫—ñ–ª—å–∫—ñ—Å—Ç—å –º—ñ—Å—Ü—å - \d+\.?', '', text)
#         hostel_info.append(text)
#
#     result = {
#         'description': first_paragraph,
#         'hostels': hostel_info
#     }
#     logging.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –≥—É—Ä—Ç–æ–∂–∏—Ç–∫–∏: {result}")
#     return result
#
# def stolova() -> str:
#     """–ü–∞—Ä—Å–∏—Ç—å —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ —ó–¥–∞–ª—å–Ω—ñ."""
#     url = "https://www.lute.lviv.ua/structure/other-units/zakladi-kharchuvannja/?L=458"
#     html = _fetch_html(url)
#     if not html:
#         return ""
#     soup = BeautifulSoup(html, 'html.parser')
#     div_content = soup.select('p.bodytext')
#     cleaned_res = [
#         p.get_text(strip=True)
#         for p in div_content
#         if p.get_text(strip=True) and p.get_text(strip=True) != '–ü–æ—Å—ñ–¥–∞—î 26 –º—ñ—Å—Ü–µ –≤ –£–∫—Ä–∞—ó–Ω—ñ'
#     ]
#     formatted_text = "üçΩ <b>–Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ —ó–¥–∞–ª—å–Ω—ñ:</b>\n\n" + "\n\n".join(
#         f"‚Ä¢ {item}" for item in cleaned_res
#     )
#     logging.info("–û—Ç—Ä–∏–º–∞–Ω–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ —ó–¥–∞–ª—å–Ω—ñ.")
#     return formatted_text
#
# def magistr_specialitation() -> list[str]:
#     """–ü–∞—Ä—Å–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–æ—Å—Ç–µ–π –º–∞–≥—ñ—Å—Ç—Ä–∞."""
#     url = "https://www.lute.lviv.ua/admissions/osvitnii-riven-magistr/specialnist/?L=7341"
#     html = _fetch_html(url)
#     if not html:
#         return []
#     soup = BeautifulSoup(html, 'html.parser')
#     div_content = soup.select('#c11982')
#     if div_content:
#         raw_text = div_content[0].get_text()
#         specialties = re.split(r'(?=[A-Z]\d+ - )', raw_text)
#         cleaned_specialties = [spec.strip() for spec in specialties if spec.strip() and not spec.isspace()]
#         logging.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–æ—Å—Ç—ñ –º–∞–≥—ñ—Å—Ç—Ä–∞: {cleaned_specialties}")
#         return cleaned_specialties
#     else:
#         logging.warning("–ë–ª–æ–∫ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–æ—Å—Ç–µ–π –º–∞–≥—ñ—Å—Ç—Ä–∞ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.")
#         return []
#
# def bakalavr_specialitation() -> list[str]:
#     """–ü–∞—Ä—Å–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–æ—Å—Ç–µ–π –±–∞–∫–∞–ª–∞–≤—Ä–∞."""
#     url = "https://www.lute.lviv.ua/admissions/gss/sp/?L=60"
#     html = _fetch_html(url)
#     if not html:
#         return []
#     soup = BeautifulSoup(html, 'html.parser')
#     div_content = soup.select('#c629')
#     if div_content:
#         raw_text = div_content[0].get_text()
#         specialties = re.split(r'(?=[A-Z]\d+ - )', raw_text)
#         cleaned_specialties = [spec.strip() for spec in specialties if spec.strip() and not spec.isspace()]
#         logging.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–æ—Å—Ç—ñ –±–∞–∫–∞–ª–∞–≤—Ä–∞: {cleaned_specialties}")
#         return cleaned_specialties
#     else:
#         logging.warning("–ë–ª–æ–∫ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–æ—Å—Ç–µ–π –±–∞–∫–∞–ª–∞–≤—Ä–∞ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.")
#         return []
#
# def main():
#     parse_priyomna_numbers()
#     parse_priyomna_info()
#     get_rektor()
#     parse_hostel_info()
#     print(stolova())
#     magistr_specialitation()
#     bakalavr_specialitation()
#
# if __name__ == '__main__':
#     main()

from bs4 import BeautifulSoup
import requests
import fake_useragent
import re
import os
import json
from datetime import datetime, timedelta
import logging

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ª–æ–≥—É–≤–∞–Ω–Ω—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

CACHE_DIR = 'cache'
CACHE_EXPIRATION_TIME = timedelta(hours=1)

def _sanitize_filename(url: str) -> str:
    """–û—á–∏—â—É—î URL –¥–ª—è –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –≤ —è–∫–æ—Å—Ç—ñ —ñ–º–µ–Ω—ñ —Ñ–∞–π–ª—É –∫–µ—à—É."""
    return url.replace('https://', '').replace('/', '_').replace(':', '_').replace('?', '_').replace('=', '_') + '.json'

def _fetch_html_with_cache(url: str) -> str:
    """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î HTML-–∫–æ–Ω—Ç–µ–Ω—Ç –∑ –≤–∫–∞–∑–∞–Ω–æ–≥–æ URL –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º –∫–µ—à—É–≤–∞–Ω–Ω—è."""
    os.makedirs(CACHE_DIR, exist_ok=True)
    cache_file = os.path.join(CACHE_DIR, _sanitize_filename(url))

    if os.path.exists(cache_file):
        try:
            with open(cache_file, 'r') as f:
                cache_data = json.load(f)
                if datetime.now() - datetime.fromisoformat(cache_data['timestamp']) < CACHE_EXPIRATION_TIME:
                    logging.info(f"–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –∫–µ—à–æ–≤–∞–Ω–∞ –≤–µ—Ä—Å—ñ—è –¥–ª—è {url}")
                    return cache_data['content']
                else:
                    logging.info(f"–ö–µ—à –∑–∞—Å—Ç–∞—Ä—ñ–≤ –¥–ª—è {url}, –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –Ω–æ–≤—ñ –¥–∞–Ω—ñ")
        except (FileNotFoundError, json.JSONDecodeError):
            logging.warning(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —á–∏—Ç–∞–Ω–Ω—ñ –∫–µ—à-—Ñ–∞–π–ª—É –¥–ª—è {url}, –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –Ω–æ–≤—ñ –¥–∞–Ω—ñ")

    user = fake_useragent.UserAgent().random
    headers = {'user-agent': user}
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()  # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –ø–æ–º–∏–ª–∫–∏ HTTP
        content = response.text
        cache_data = {'timestamp': datetime.now().isoformat(), 'content': content}
        with open(cache_file, 'w') as f:
            json.dump(cache_data, f)
        return content
    except requests.exceptions.RequestException as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—ñ {url}: {e}")
        return ""

def parse_priyomna_numbers():
    """–ü–∞—Ä—Å–∏—Ç—å –Ω–æ–º–µ—Ä–∏ —Ç–µ–ª–µ—Ñ–æ–Ω—ñ–≤ –ø—Ä–∏–π–º–∞–ª—å–Ω–æ—ó –∫–æ–º—ñ—Å—ñ—ó –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º –∫–µ—à—É–≤–∞–Ω–Ω—è."""
    url = "https://www.lute.lviv.ua/admissions/priimalna-komisija/?L=274"
    html = _fetch_html_with_cache(url)
    if not html:
        return []
    soup = BeautifulSoup(html, 'html.parser')
    all_p = soup.find_all('p', class_='bodytext')
    res = []
    for div in all_p:
        if '+38' in div.get_text():
            numbers = div.find_all(string=True)
            res.extend(numbers)
            filtered = [item for item in res if item.strip() != '—Ç–µ–ª.:']
            formatted_numbers = []
            i = 0
            while i < len(filtered):
                if filtered[i] == '+38 ' and i + 1 < len(filtered):
                    formatted_numbers.append(f"+38 {filtered[i + 1]}")
                    i += 2
                else:
                    formatted_numbers.append(filtered[i])
                    i += 1
            final_numbers = [num.strip() for num in formatted_numbers]
            logging.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ –Ω–æ–º–µ—Ä–∏ —Ç–µ–ª–µ—Ñ–æ–Ω—ñ–≤ –ø—Ä–∏–π–º–∞–ª—å–Ω–æ—ó –∫–æ–º—ñ—Å—ñ—ó: {final_numbers}")
            return final_numbers
    return []

def parse_priyomna_info():
    """–ü–∞—Ä—Å–∏—Ç—å –∫–æ–Ω—Ç–∞–∫—Ç–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–∏–π–º–∞–ª—å–Ω–æ—ó –∫–æ–º—ñ—Å—ñ—ó –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º –∫–µ—à—É–≤–∞–Ω–Ω—è."""
    url = "https://www.lute.lviv.ua/admissions/priimalna-komisija/?L=274"
    html = _fetch_html_with_cache(url)
    if not html:
        return {}
    soup = BeautifulSoup(html, 'html.parser')
    contact_box = soup.find('div', class_='fce-box-orange')
    if not contact_box:
        logging.warning("–ë–ª–æ–∫ –∫–æ–Ω—Ç–∞–∫—Ç–Ω–æ—ó —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.")
        return {}
    address = contact_box.find('span', lang='RU').get_text(strip=True).replace('–∞–¥—Ä–µ—Å–∞:', '').strip() if contact_box.find('span', lang='RU') else None
    email_section = contact_box.find('a', class_='mail')
    email = email_section.get_text(strip=True).replace('(at)', '@') if email_section else None
    facebook = contact_box.find('a', href=re.compile('facebook.com')).get('href') if contact_box.find('a', href=re.compile('facebook.com')) else None
    telegram = contact_box.find('a', href=re.compile('t.me')).get('href') if contact_box.find('a', href=re.compile('t.me')) else None
    result = {
        'address': address,
        'email': email,
        'facebook': facebook,
        'telegram': telegram
    }
    logging.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –ø—Ä–∏–π–º–∞–ª—å–Ω—É –∫–æ–º—ñ—Å—ñ—é: {result}")
    return result

def get_rektor():
    """–ü–∞—Ä—Å–∏—Ç—å –∫–æ–Ω—Ç–∞–∫—Ç–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é —Ä–µ–∫—Ç–æ—Ä–∞ –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º –∫–µ—à—É–≤–∞–Ω–Ω—è."""
    url = "https://www.lute.lviv.ua/universitet/kontakti/"
    html = _fetch_html_with_cache(url)
    if not html:
        return {}
    soup = BeautifulSoup(html, 'html.parser')
    prinimaet_rektor = soup.find_all('div', id='c5228')

    def process_data(divs):
        data_dict = {}
        for div in divs:
            for p in div.find_all('p'):
                text = p.text.strip().replace('\xa0', ' ')
                if ":" in text:
                    key, value = text.split(":", 1)
                    key = key.strip()
                    value = value.strip()
                    if key == '—Ç–µ–ª–µ—Ñ–æ–Ω':
                        phones = re.findall(r'\(\d{3}\) \d{3}-\d{2}-\d{2}', value)
                        data_dict[key] = ', '.join(phones)
                    else:
                        data_dict[key] = value
        return data_dict

    rektor_info = process_data(prinimaet_rektor)
    logging.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ —Ä–µ–∫—Ç–æ—Ä–∞: {rektor_info}")
    return rektor_info

def parse_hostel_info():
    """–ü–∞—Ä—Å–∏—Ç—å —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –≥—É—Ä—Ç–æ–∂–∏—Ç–∫–∏ –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º –∫–µ—à—É–≤–∞–Ω–Ω—è."""
    url = "https://www.lute.lviv.ua/admissions/prozhivannja-studentiv/?L=458"
    html = _fetch_html_with_cache(url)
    if not html:
        return {}
    soup = BeautifulSoup(html, 'html.parser')
    content_div = soup.find('div', class_='csc-textpic-text')
    if not content_div:
        logging.warning("–ë–ª–æ–∫ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –ø—Ä–æ –≥—É—Ä—Ç–æ–∂–∏—Ç–∫–∏ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.")
        return {}
    first_paragraph = content_div.find('p', class_='bodytext').get_text(strip=True) if content_div.find('p', class_='bodytext') else None
    hostel_info = []
    for li in content_div.find_all('li', limit=3):
        text = li.get_text(strip=True).replace('\xa0', ' ')
        text = re.sub(r', –∫—ñ–ª—å–∫—ñ—Å—Ç—å –º—ñ—Å—Ü—å - \d+\.?', '', text)
        hostel_info.append(text)
    result = {
        'description': first_paragraph,
        'hostels': hostel_info
    }
    logging.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –≥—É—Ä—Ç–æ–∂–∏—Ç–∫–∏: {result}")
    return result

def stolova():
    """–ü–∞—Ä—Å–∏—Ç—å —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ —ó–¥–∞–ª—å–Ω—ñ –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º –∫–µ—à—É–≤–∞–Ω–Ω—è."""
    url = "https://www.lute.lviv.ua/structure/other-units/zakladi-kharchuvannja/?L=458"
    html = _fetch_html_with_cache(url)
    if not html:
        return ""
    soup = BeautifulSoup(html, 'html.parser')
    div_content = soup.find_all('p', class_='bodytext')
    cleaned_res = [
        p.get_text(strip=True)
        for p in div_content
        if p.get_text(strip=True) and p.get_text(strip=True) != '–ü–æ—Å—ñ–¥–∞—î 26 –º—ñ—Å—Ü–µ –≤ –£–∫—Ä–∞—ó–Ω—ñ'
    ]
    formatted_text = "üçΩ <b>–Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ —ó–¥–∞–ª—å–Ω—ñ:</b>\n\n" + "\n\n".join(
        f"‚Ä¢ {item}" for item in cleaned_res
    )
    logging.info("–û—Ç—Ä–∏–º–∞–Ω–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ —ó–¥–∞–ª—å–Ω—ñ.")
    return formatted_text

def magistr_specialitation():
    """–ü–∞—Ä—Å–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–æ—Å—Ç–µ–π –º–∞–≥—ñ—Å—Ç—Ä–∞ –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º –∫–µ—à—É–≤–∞–Ω–Ω—è."""
    url = "https://www.lute.lviv.ua/admissions/osvitnii-riven-magistr/specialnist/?L=7341"
    html = _fetch_html_with_cache(url)
    if not html:
        return []
    soup = BeautifulSoup(html, 'html.parser')
    div_content = soup.find_all('div', id='c11982')
    if div_content:
        raw_text = div_content[0].get_text()
        specialties = re.split(r'(?=[A-Z]\d+ - )', raw_text)
        cleaned_specialties = [spec.strip() for spec in specialties if spec.strip() and not spec.isspace()]
        logging.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–æ—Å—Ç—ñ –º–∞–≥—ñ—Å—Ç—Ä–∞: {cleaned_specialties}")
        return cleaned_specialties
    else:
        logging.warning("–ë–ª–æ–∫ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–æ—Å—Ç–µ–π –º–∞–≥—ñ—Å—Ç—Ä–∞ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.")
        return []

def bakalavr_specialitation():
    """–ü–∞—Ä—Å–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–æ—Å—Ç–µ–π –±–∞–∫–∞–ª–∞–≤—Ä–∞ –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º –∫–µ—à—É–≤–∞–Ω–Ω—è."""
    url = "https://www.lute.lviv.ua/admissions/gss/sp/?L=60"
    html = _fetch_html_with_cache(url)
    if not html:
        return []
    soup = BeautifulSoup(html, 'html.parser')
    div_content = soup.find_all('div', id='c629')
    if div_content:
        raw_text = div_content[0].get_text()
        specialties = re.split(r'(?=[A-Z]\d+ - )', raw_text)
        cleaned_specialties = [spec.strip() for spec in specialties if spec.strip() and not spec.isspace()]
        logging.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–æ—Å—Ç—ñ –±–∞–∫–∞–ª–∞–≤—Ä–∞: {cleaned_specialties}")
        return cleaned_specialties
    else:
        logging.warning("–ë–ª–æ–∫ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–æ—Å—Ç–µ–π –±–∞–∫–∞–ª–∞–≤—Ä–∞ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.")
        return []

def main():
    parse_priyomna_numbers()

if __name__ == '__main__':
    main()