from faker import Faker
from bs4 import BeautifulSoup
import requests
import fake_useragent
import re
from datetime import datetime
import pytz

def parse_priyomna_numbers():

    user = fake_useragent.UserAgent().random
    headers = {'user-agent': user}
    url = "https://www.lute.lviv.ua/admissions/priimalna-komisija/?L=274"
    response = requests.get(url, headers=headers).text
    soup = BeautifulSoup(response, 'html.parser')

    all_p = soup.find_all('p', class_='bodytext')

    res = []

    for div in all_p:
        if '+38' in div.get_text():
            numbers = div.find_all(string=True)  # –ø–æ–ª—É—á–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç
            res.extend(numbers)
            print(res)
            # –£–¥–∞–ª—è–µ–º —ç–ª–µ–º–µ–Ω—Ç '—Ç–µ–ª.:'
            filtered = [item for item in res if item.strip() != '—Ç–µ–ª.:']
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º '+38' —Å–æ —Å–ª–µ–¥—É—é—â–∏–º–∏ —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏
            formatted_numbers = []
            i = 0
            while i < len(filtered):
                if filtered[i] == '+38 ' and i + 1 < len(filtered):
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º '+38' —Å–æ —Å–ª–µ–¥—É—é—â–∏–º —ç–ª–µ–º–µ–Ω—Ç–æ–º
                    formatted_numbers.append(f"+38 {filtered[i + 1]}")
                    i += 2
                else:
                    # –ü—Ä–æ—Å—Ç–æ –¥–æ–±–∞–≤–ª—è–µ–º —ç–ª–µ–º–µ–Ω—Ç –∫–∞–∫ –µ—Å—Ç—å (—É–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç +38)
                    formatted_numbers.append(filtered[i])
                    i += 1
            final_numbers = [num.strip() for num in formatted_numbers]
            print(final_numbers)

    return final_numbers

def parse_priyomna_info():
    user = fake_useragent.UserAgent().random
    headers = {'user-agent': user}
    url = "https://www.lute.lviv.ua/admissions/priimalna-komisija/?L=274"
    response = requests.get(url, headers=headers).text
    soup = BeautifulSoup(response, 'html.parser')

    contact_box = soup.find('div', class_='fce-box-orange')

    # –ü–∞—Ä—Å–∏–º –∞–¥—Ä–µ—Å
    address = contact_box.find('span', lang='RU').get_text(strip=True).replace('–∞–¥—Ä–µ—Å–∞:', '').strip()

    # –ü–∞—Ä—Å–∏–º email
    email_section = contact_box.find('a', class_='mail')
    email = email_section.get_text(strip=True).replace('(at)', '@')

    # –ü–∞—Ä—Å–∏–º Facebook –∏ Telegram
    facebook = contact_box.find('a', href=re.compile('facebook.com')).get('href')
    telegram = contact_box.find('a', href=re.compile('t.me')).get('href')

    # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    result = {
        'address': address,
        'email': email,
        'facebook': facebook,
        'telegram': telegram
    }
    print(result)
    return result

def get_rektor():
    # –í–∞—à –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
    user = fake_useragent.UserAgent().random
    headers = {'user-agent': user}
    url = "https://www.lute.lviv.ua/universitet/kontakti/"
    response = requests.get(url, headers=headers).text
    soup = BeautifulSoup(response, 'html.parser')
    prinimaet_rektor = soup.find_all('div', id='c5228')
    prinimaet_komisiya = soup.find_all('div', id='c5232')

    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
    def process_data(divs):
        result = []
        for div in divs:
            num = div.find_all('p')
            for p in num:
                text = p.text.strip()
                text = text.replace('\xa0', ' ')
                text = ' '.join(text.split())
                result.append(text)

        data_dict = {}
        for item in result:
            key, value = item.split(":", 1)
            key = key.strip()
            value = value.strip()

            # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–ª–µ—Ñ–æ–Ω–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤
            if key == '—Ç–µ–ª–µ—Ñ–æ–Ω':
                # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –Ω–æ–º–µ—Ä–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ (XXX) XXX-XX-XX
                phones = re.findall(r'\(\d{3}\) \d{3}-\d{2}-\d{2}', value)
                value = ', '.join(phones)

            data_dict[key] = value

        return data_dict

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ä–µ–∫—Ç–æ—Ä–∞ –∏ –∫–æ–º–∏—Å—Å–∏–∏
    dict_prinimaet_rektor = process_data(prinimaet_rektor)

    return dict_prinimaet_rektor

# —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ –≥—É—Ä—Ç–æ–∂–∏—Ç–æ–∫
def parse_hostel_info():
    url = "https://www.lute.lviv.ua/admissions/prozhivannja-studentiv/?L=458"
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')

    # –ù–∞—Ö–æ–¥–∏–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç–Ω—ã–π –±–ª–æ–∫
    content_div = soup.find('div', class_='csc-textpic-text')

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–µ—Ä–≤—ã–π –∞–±–∑–∞—Ü (–ø–µ—Ä–≤—ã–π <p> —Å –∫–ª–∞—Å—Å–æ–º bodytext)
    first_paragraph = content_div.find('p', class_='bodytext').get_text(strip=True)

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–µ—Ä–≤—ã–µ 3 –ø—É–Ω–∫—Ç–∞ —Å–ø–∏—Å–∫–∞ –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏—Ö
    list_items = content_div.find_all('li', limit=3)
    hostel_info = []
    for li in list_items:
        text = li.get_text(strip=True)
        # –£–¥–∞–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –º–µ—Å—Ç
        text = re.sub(r', –∫—ñ–ª—å–∫—ñ—Å—Ç—å –º—ñ—Å—Ü—å - \d+\.?', '', text)
        # –ó–∞–º–µ–Ω—è–µ–º –Ω–µ—Ä–∞–∑—Ä—ã–≤–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
        text = text.replace('\xa0', ' ')
        hostel_info.append(text)

    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    result = {
        'description': first_paragraph,
        'hostels': hostel_info
    }
    print(result)
    return result


# –Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ —ó–¥–∞–ª—å–Ω—é
def stolova():

    user = fake_useragent.UserAgent().random
    headers = {'user-agent': user}
    url = "https://www.lute.lviv.ua/structure/other-units/zakladi-kharchuvannja/?L=458"
    response = requests.get(url, headers=headers).text
    soup = BeautifulSoup(response, 'html.parser')

    div_content = soup.find_all('p', class_='bodytext')
    res = []

    for p in div_content:
        text = p.get_text(strip=True)
        res.append(text)

    cleaned_res = [
        item for item in res
        if item and item != '–ü–æ—Å—ñ–¥–∞—î 26 –º—ñ—Å—Ü–µ –≤ –£–∫—Ä–∞—ó–Ω—ñ'
    ]

    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤ —Å—Ç—Ä–æ–∫—É —Å —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
    formatted_text = "üçΩ <b>–Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ —ó–¥–∞–ª—å–Ω—ñ:</b>\n\n" + "\n\n".join(
        f"‚Ä¢ {item}" for item in cleaned_res
    )

    return formatted_text  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ç—Ä–æ–∫—É, –∞ –Ω–µ —Å–ø–∏—Å–æ–∫

#—ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–æ—Å—Ç—ñ –º–∞–≥—ñ—Å—Ç—Ä
def magistr_specialitation():

    user = fake_useragent.UserAgent().random
    headers = {'user-agent': user}
    url = "https://www.lute.lviv.ua/admissions/osvitnii-riven-magistr/specialnist/?L=7341"
    response = requests.get(url, headers=headers).text
    soup = BeautifulSoup(response, 'html.parser')
    div_content = soup.find_all('div', id='c11982')
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º
    raw_text = div_content[0].get_text() if div_content else ""

    # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ —à–∞–±–ª–æ–Ω—É "–ë—É–∫–≤–∞–¶–∏—Ñ—Ä–∞ -"
    import re
    specialties = re.split(r'(?=[A-Z]\d+ - )', raw_text)

    # –§–∏–ª—å—Ç—Ä—É–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –∏ –æ—á–∏—â–∞–µ–º
    cleaned_specialties = [
        spec.strip()
        for spec in specialties
        if spec.strip() and not spec.isspace()
    ]
    print(cleaned_specialties)
    return cleaned_specialties

#—ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–æ—Å—Ç—ñ –±–∞–∫–∞–ª–∞–≤—Ä
def bakalavr_specialitation():

    user = fake_useragent.UserAgent().random
    headers = {'user-agent': user}
    url = "https://www.lute.lviv.ua/admissions/gss/sp/?L=60"
    response = requests.get(url, headers=headers).text
    soup = BeautifulSoup(response, 'html.parser')
    div_content = soup.find_all('div', id='c629')
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º
    raw_text = div_content[0].get_text() if div_content else ""

    # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ —à–∞–±–ª–æ–Ω—É "–ë—É–∫–≤–∞–¶–∏—Ñ—Ä–∞ -"
    import re
    specialties = re.split(r'(?=[A-Z]\d+ - )', raw_text)

    # –§–∏–ª—å—Ç—Ä—É–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –∏ –æ—á–∏—â–∞–µ–º
    cleaned_specialties = [
        spec.strip()
        for spec in specialties
        if spec.strip() and not spec.isspace()
    ]
    print(cleaned_specialties)
    return cleaned_specialties

def get_now_time():
    now = datetime.now(pytz.timezone('Europe/Kiev'))
    return now.replace(tzinfo=None)


# def main():
#     magistr_specialitation()
#
# if __name__ == '__main__':
#     main()