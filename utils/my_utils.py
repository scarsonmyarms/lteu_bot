from bs4 import BeautifulSoup
import requests
import fake_useragent
import re
import os
import json
from datetime import datetime, timedelta
import logging
import pytz

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ª–æ–≥—É–≤–∞–Ω–Ω—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

CACHE_DIR = 'cache'
CACHE_EXPIRATION_TIME = timedelta(hours=1)

def _sanitize_filename(url: str) -> str:
    """–û—á–∏—â—É—î URL –¥–ª—è –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –≤ —è–∫–æ—Å—Ç—ñ —ñ–º–µ–Ω—ñ —Ñ–∞–π–ª—É –∫–µ—à—É."""
    return url.replace('https://', '').replace('/', '_').replace(':', '_').replace('?', '_').replace('=', '_') + '.json'

def _fetch_html_with_cache(url: str) -> str:
    """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î HTML-–∫–æ–Ω—Ç–µ–Ω—Ç –∑ –≤–∫–∞–∑–∞–Ω–æ–≥–æ URL –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º –∫–µ—à—É–≤–∞–Ω–Ω—è."""
    os.makedirs(CACHE_DIR, exist_ok=True)
    cache_file = os.path.join(CACHE_DIR, _sanitize_filename(url))

    if os.path.exists(cache_file):
        try:
            with open(cache_file, 'r') as f:
                cache_data = json.load(f)
                if datetime.now() - datetime.fromisoformat(cache_data['timestamp']) < CACHE_EXPIRATION_TIME:
                    logging.info(f"–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –∫–µ—à–æ–≤–∞–Ω–∞ –≤–µ—Ä—Å—ñ—è –¥–ª—è {url}")
                    return cache_data['content']
                else:
                    logging.info(f"–ö–µ—à –∑–∞—Å—Ç–∞—Ä—ñ–≤ –¥–ª—è {url}, –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –Ω–æ–≤—ñ –¥–∞–Ω—ñ")
        except (FileNotFoundError, json.JSONDecodeError):
            logging.warning(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —á–∏—Ç–∞–Ω–Ω—ñ –∫–µ—à-—Ñ–∞–π–ª—É –¥–ª—è {url}, –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –Ω–æ–≤—ñ –¥–∞–Ω—ñ")

    user = fake_useragent.UserAgent().random
    headers = {'user-agent': user}
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()  # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –ø–æ–º–∏–ª–∫–∏ HTTP
        content = response.text
        cache_data = {'timestamp': datetime.now().isoformat(), 'content': content}
        with open(cache_file, 'w') as f:
            json.dump(cache_data, f)
        return content
    except requests.exceptions.RequestException as e:
        logging.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—ñ {url}: {e}")
        return ""

def parse_priyomna_numbers():
    # –ü–∞—Ä—Å–∏—Ç—å –Ω–æ–º–µ—Ä–∏ —Ç–µ–ª–µ—Ñ–æ–Ω—ñ–≤ –ø—Ä–∏–π–º–∞–ª—å–Ω–æ—ó –∫–æ–º—ñ—Å—ñ—ó
    url = "https://www.lute.lviv.ua/admissions/priimalna-komisija/?L=274"
    html = _fetch_html_with_cache(url)
    if not html:
        return []
    soup = BeautifulSoup(html, 'html.parser')
    all_p = soup.find_all('p', class_='bodytext')
    res = []
    for div in all_p:
        if '+38' in div.get_text():
            numbers = div.find_all(string=True)
            res.extend(numbers)
            filtered = [item for item in res if item.strip() != '—Ç–µ–ª.:']
            formatted_numbers = []
            i = 0
            while i < len(filtered):
                if filtered[i] == '+38 ' and i + 1 < len(filtered):
                    formatted_numbers.append(f"+38 {filtered[i + 1]}")
                    i += 2
                else:
                    formatted_numbers.append(filtered[i])
                    i += 1
            final_numbers = [num.strip() for num in formatted_numbers]
            logging.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ –Ω–æ–º–µ—Ä–∏ —Ç–µ–ª–µ—Ñ–æ–Ω—ñ–≤ –ø—Ä–∏–π–º–∞–ª—å–Ω–æ—ó –∫–æ–º—ñ—Å—ñ—ó: {final_numbers}")
            return final_numbers
    return []

def parse_priyomna_info():
    """–ü–∞—Ä—Å–∏—Ç—å –∫–æ–Ω—Ç–∞–∫—Ç–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–∏–π–º–∞–ª—å–Ω–æ—ó –∫–æ–º—ñ—Å—ñ—ó –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º –∫–µ—à—É–≤–∞–Ω–Ω—è."""
    url = "https://www.lute.lviv.ua/admissions/priimalna-komisija/?L=274"
    html = _fetch_html_with_cache(url)
    if not html:
        return {}
    soup = BeautifulSoup(html, 'html.parser')
    contact_box = soup.find('div', class_='fce-box-orange')
    if not contact_box:
        logging.warning("–ë–ª–æ–∫ –∫–æ–Ω—Ç–∞–∫—Ç–Ω–æ—ó —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.")
        return {}
    address = contact_box.find('span', lang='RU').get_text(strip=True).replace('–∞–¥—Ä–µ—Å–∞:', '').strip() if contact_box.find('span', lang='RU') else None
    email_section = contact_box.find('a', class_='mail')
    email = email_section.get_text(strip=True).replace('(at)', '@') if email_section else None
    facebook = contact_box.find('a', href=re.compile('facebook.com')).get('href') if contact_box.find('a', href=re.compile('facebook.com')) else None
    telegram = contact_box.find('a', href=re.compile('t.me')).get('href') if contact_box.find('a', href=re.compile('t.me')) else None
    result = {
        'address': address,
        'email': email,
        'facebook': facebook,
        'telegram': telegram
    }
    logging.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –ø—Ä–∏–π–º–∞–ª—å–Ω—É –∫–æ–º—ñ—Å—ñ—é: {result}")
    return result

def get_rektor():
    """–ü–∞—Ä—Å–∏—Ç—å –∫–æ–Ω—Ç–∞–∫—Ç–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é —Ä–µ–∫—Ç–æ—Ä–∞ –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º –∫–µ—à—É–≤–∞–Ω–Ω—è."""
    url = "https://www.lute.lviv.ua/universitet/kontakti/"
    html = _fetch_html_with_cache(url)
    if not html:
        return {}
    soup = BeautifulSoup(html, 'html.parser')
    prinimaet_rektor = soup.find_all('div', id='c5228')

    def process_data(divs):
        data_dict = {}
        for div in divs:
            for p in div.find_all('p'):
                text = p.text.strip().replace('\xa0', ' ')
                if ":" in text:
                    key, value = text.split(":", 1)
                    key = key.strip()
                    value = value.strip()
                    if key == '—Ç–µ–ª–µ—Ñ–æ–Ω':
                        phones = re.findall(r'\(\d{3}\) \d{3}-\d{2}-\d{2}', value)
                        data_dict[key] = ', '.join(phones)
                    else:
                        data_dict[key] = value
        return data_dict

    rektor_info = process_data(prinimaet_rektor)
    logging.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ —Ä–µ–∫—Ç–æ—Ä–∞: {rektor_info}")
    return rektor_info

def parse_hostel_info():
    """–ü–∞—Ä—Å–∏—Ç—å —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –≥—É—Ä—Ç–æ–∂–∏—Ç–∫–∏ –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º –∫–µ—à—É–≤–∞–Ω–Ω—è."""
    url = "https://www.lute.lviv.ua/admissions/prozhivannja-studentiv/?L=458"
    html = _fetch_html_with_cache(url)
    if not html:
        return {}
    soup = BeautifulSoup(html, 'html.parser')
    content_div = soup.find('div', class_='csc-textpic-text')
    if not content_div:
        logging.warning("–ë–ª–æ–∫ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –ø—Ä–æ –≥—É—Ä—Ç–æ–∂–∏—Ç–∫–∏ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.")
        return {}
    first_paragraph = content_div.find('p', class_='bodytext').get_text(strip=True) if content_div.find('p', class_='bodytext') else None
    hostel_info = []
    for li in content_div.find_all('li', limit=3):
        text = li.get_text(strip=True).replace('\xa0', ' ')
        text = re.sub(r', –∫—ñ–ª—å–∫—ñ—Å—Ç—å –º—ñ—Å—Ü—å - \d+\.?', '', text)
        hostel_info.append(text)
    result = {
        'description': first_paragraph,
        'hostels': hostel_info
    }
    logging.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –≥—É—Ä—Ç–æ–∂–∏—Ç–∫–∏: {result}")
    return result

def stolova():
    """–ü–∞—Ä—Å–∏—Ç—å —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ —ó–¥–∞–ª—å–Ω—ñ –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º –∫–µ—à—É–≤–∞–Ω–Ω—è."""
    url = "https://www.lute.lviv.ua/structure/other-units/zakladi-kharchuvannja/?L=458"
    html = _fetch_html_with_cache(url)
    if not html:
        return ""
    soup = BeautifulSoup(html, 'html.parser')
    div_content = soup.find_all('p', class_='bodytext')
    cleaned_res = [
        p.get_text(strip=True)
        for p in div_content
        if p.get_text(strip=True) and p.get_text(strip=True) != '–ü–æ—Å—ñ–¥–∞—î 26 –º—ñ—Å—Ü–µ –≤ –£–∫—Ä–∞—ó–Ω—ñ'
    ]
    formatted_text = "üçΩ <b>–Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ —ó–¥–∞–ª—å–Ω—ñ:</b>\n\n" + "\n\n".join(
        f"‚Ä¢ {item}" for item in cleaned_res
    )
    logging.info("–û—Ç—Ä–∏–º–∞–Ω–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ —ó–¥–∞–ª—å–Ω—ñ.")
    return formatted_text

def magistr_specialitation():
    """–ü–∞—Ä—Å–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–æ—Å—Ç–µ–π –º–∞–≥—ñ—Å—Ç—Ä–∞ –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º –∫–µ—à—É–≤–∞–Ω–Ω—è."""
    url = "https://www.lute.lviv.ua/admissions/osvitnii-riven-magistr/specialnist/?L=7341"
    html = _fetch_html_with_cache(url)
    if not html:
        return []
    soup = BeautifulSoup(html, 'html.parser')
    div_content = soup.find_all('div', id='c11982')
    if div_content:
        raw_text = div_content[0].get_text()
        specialties = re.split(r'(?=[A-Z]\d+ - )', raw_text)
        cleaned_specialties = [spec.strip() for spec in specialties if spec.strip() and not spec.isspace()]
        logging.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–æ—Å—Ç—ñ –º–∞–≥—ñ—Å—Ç—Ä–∞: {cleaned_specialties}")
        return cleaned_specialties
    else:
        logging.warning("–ë–ª–æ–∫ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–æ—Å—Ç–µ–π –º–∞–≥—ñ—Å—Ç—Ä–∞ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.")
        return []

def bakalavr_specialitation():
    """–ü–∞—Ä—Å–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–æ—Å—Ç–µ–π –±–∞–∫–∞–ª–∞–≤—Ä–∞ –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º –∫–µ—à—É–≤–∞–Ω–Ω—è."""
    url = "https://www.lute.lviv.ua/admissions/gss/sp/?L=60"
    html = _fetch_html_with_cache(url)
    if not html:
        return []
    soup = BeautifulSoup(html, 'html.parser')
    div_content = soup.find_all('div', id='c629')
    if div_content:
        raw_text = div_content[0].get_text()
        specialties = re.split(r'(?=[A-Z]\d+ - )', raw_text)
        cleaned_specialties = [spec.strip() for spec in specialties if spec.strip() and not spec.isspace()]
        logging.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–æ—Å—Ç—ñ –±–∞–∫–∞–ª–∞–≤—Ä–∞: {cleaned_specialties}")
        return cleaned_specialties
    else:
        logging.warning("–ë–ª–æ–∫ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–æ—Å—Ç–µ–π –±–∞–∫–∞–ª–∞–≤—Ä–∞ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.")
        return []

def get_now_time():
    now = datetime.now(pytz.timezone('Europe/Kiev'))
    return now.replace(tzinfo=None)

# –∑–∞ –ø–æ—Ç—Ä–µ–±–∏ –≤–∏–¥–∞–ª–∏—Ç–∏ –Ω–µ –ø–æ—Ç—Ä—ñ–±–Ω—ñ –≤–∏–≤–æ–¥–∏
def main():
    parse_priyomna_numbers()
    bakalavr_specialitation()
    stolova()
    parse_hostel_info()
    get_rektor()
    parse_priyomna_info()
    parse_priyomna_numbers()
    get_now_time()

if __name__ == '__main__':
    main()
